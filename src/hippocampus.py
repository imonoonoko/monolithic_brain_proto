import numpy as np
import json
import os
import uuid
import base64
from datetime import datetime
from typing import Dict, List, Optional, Tuple

class Hippocampus:
    """
    æµ·é¦¬ (Hippocampus) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    LLMã®æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆLogprobsï¼‰ã‚’æ•°å­¦çš„ã«æŠ•å½±ã—ã€è¿½åŠ ã‚³ã‚¹ãƒˆã€Œã‚¼ãƒ­ã€ã§
    æ€è€ƒãƒ™ã‚¯ãƒˆãƒ«ï¼ˆSemantic Hypervectorï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    def __init__(self, vocab_size: int = 152064, hdc_dim: int = 4096, seed: int = 42):
        """
        Args:
            vocab_size: ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚º (Qwen2.5-1.5B ã¯ ~152k)
            hdc_dim: HDCãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ å°„å½±è¡Œåˆ—ã®å›ºå®šã‚·ãƒ¼ãƒ‰
        """
        # å†ç¾æ€§ã®ãŸã‚ã‚·ãƒ¼ãƒ‰å›ºå®š
        np.random.seed(seed)
        
        # å°„å½±è¡Œåˆ—: ã‚¹ãƒ‘ãƒ¼ã‚¹ãªLogprobsã‚’å¯†ãªHDCãƒ™ã‚¯ãƒˆãƒ«ã¸å¤‰æ›
        # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€ä½¿ç”¨æ™‚ã«ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ç”Ÿæˆã™ã‚‹ã‹ã€ã¾ãŸã¯è»½é‡ãªãƒãƒƒã‚·ãƒ¥é–¢æ•°ã§ä»£ç”¨ã‚‚æ¤œè¨å¯èƒ½ã ãŒã€
        # ã“ã“ã§ã¯å˜ç´”åŒ–ã®ãŸã‚è¡Œåˆ—ã‚’æŒã¤ (ãŸã ã—ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã®ãŸã‚è¡Œåˆ—å…¨ä½“ã¯ä¿æŒã›ãšã¨ã‚‚è‰¯ã„ãŒã€å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ä¿æŒ)
        # Note: 152k * 4k * float16 ã¯å·¨å¤§(1.2GB)ã«ãªã‚‹ãŸã‚ã€
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ã€Œãƒãƒƒã‚·ãƒ¥å°„å½±ã€ã¾ãŸã¯ã€Œå›ºå®šãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‹ã‚‰ã®å‹•çš„ç”Ÿæˆã€ãŒç†æƒ³ã€‚
        # ä»Šå›ã¯MVPã¨ã—ã¦ã€ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–æ™‚ã«è¡Œåˆ—ã‚’æŒãŸãšã€encodeæ™‚ã«å‹•çš„ã«è¨ˆç®—ã™ã‚‹ã€Œæ“¬ä¼¼å°„å½±ã€ã‚’æ¡ç”¨ã™ã‚‹ã€‚
        # (å·¨å¤§ãªè¡Œåˆ—ã‚’æŒã¤ã¨ãƒ¡ãƒ¢ãƒªåœ§è¿«ã®åŸå› ã«ãªã‚‹ãŸã‚)
        
        self.vocab_size = vocab_size
        self.hdc_dim = hdc_dim
        self.seed = seed

    def project_thought(self, top_logprobs: Dict[str, float]) -> np.ndarray:
        """
        Logprobs (Top-K thinking pattern) ã‚’æ€è€ƒãƒ™ã‚¯ãƒˆãƒ«ã«å°„å½±ã™ã‚‹ã€‚
        è¡Œåˆ—ã‚’æŒãŸãšã€ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ã‚·ãƒ¼ãƒ‰ã¨ã—ãŸä¹±æ•°ç”Ÿæˆã§å°„å½±ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã»ã¼ã‚¼ãƒ­ï¼‰ã€‚
        """
        thought_vector = np.zeros(self.hdc_dim, dtype=np.float32)
        
        # ç¢ºç‡åˆ†å¸ƒã®æ­£è¦åŒ–
        # APIã‹ã‚‰ã¯å¯¾æ•°ç¢ºç‡ãŒæ¥ã‚‹ -> ç¢ºç‡ã«æˆ»ã™
        log_probs = np.array(list(top_logprobs.values()))
        token_strs = list(top_logprobs.keys())
        
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®Maxå¼•ã
        probs = np.exp(log_probs - np.max(log_probs))
        probs = probs / (np.sum(probs) + 1e-10) # æ­£è¦åŒ–
        
        for i, token_str in enumerate(token_strs):
            p = probs[i]
            if p < 0.01: continue # å½±éŸ¿ã®å°ã•ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¯ç„¡è¦–ã—ã¦é«˜é€ŸåŒ–
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ã‚·ãƒ¼ãƒ‰ã«ã™ã‚‹ï¼ˆä¸€è²«æ€§ç¢ºä¿ï¼‰
            # æ³¨æ„: æ–‡å­—åˆ—ãã®ã‚‚ã®ã‚’ä½¿ã†ã“ã¨ã§ã€Tokenizerã®IDå¤‰æ›´ã«å¼·ããªã‚‹
            token_seed = hash(token_str) % (2**32)
            
            # ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³å›ºæœ‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‹•çš„ç”Ÿæˆ (æ­£è¦åˆ†å¸ƒ)
            rng = np.random.RandomState(token_seed)
            # ãƒã‚¤ãƒãƒ¼ãƒ© (-1, 1) ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦åŠ ç®—
            # å…¨æ¬¡å…ƒä½œã‚‹ã¨é‡ã„ã®ã§ã€éã‚¼ãƒ­è¦ç´ ã®ã¿ã‚’é¸æŠçš„ã«åŠ ç®—ã™ã‚‹ã€ŒSparse Codingã€çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚‚å¯ã ãŒ
            # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆã‚µã‚¤ã‚ºå°ãªã‚‰é«˜é€Ÿï¼‰
            token_vector = rng.choice([-1.0, 1.0], size=self.hdc_dim)
            
            thought_vector += p * token_vector
            
        # äºŒå€¤åŒ– (Bipolarize) ã—ã¦HDCã®ç‰¹æ€§ï¼ˆãƒã‚¤ã‚ºè€æ€§ï¼‰ã‚’å¾—ã‚‹
        # 0ä»¥ä¸Šãªã‚‰1, æœªæº€ãªã‚‰-1
        bipolar_vector = np.where(thought_vector >= 0, 1.0, -1.0)
        
        return bipolar_vector

    def cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """
        2ã¤ã®æ€è€ƒãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®— (-1.0 ~ 1.0)
        """
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return np.dot(v1, v2) / (norm1 * norm2)

    # =========================================
    # é•·æœŸè¨˜æ†¶ (LTM) æ°¸ç¶šåŒ–æ©Ÿèƒ½
    # =========================================
    
    def _encode_vector(self, vec: np.ndarray) -> str:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚’Base64æ–‡å­—åˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡åŒ–ï¼‰"""
        return base64.b64encode(vec.astype(np.float32).tobytes()).decode('ascii')
    
    def _decode_vector(self, encoded: str) -> np.ndarray:
        """Base64æ–‡å­—åˆ—ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        return np.frombuffer(base64.b64decode(encoded), dtype=np.float32)
    
    def save_memory(
        self, 
        vector: np.ndarray, 
        user_input: str, 
        response: str, 
        filepath: str,
        importance: float = 0.5
    ) -> str:
        """
        æ€è€ƒãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’LTMã«ä¿å­˜ã™ã‚‹ã€‚
        
        Args:
            vector: 4096dim HDCãƒ™ã‚¯ãƒˆãƒ«
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±
            response: NPCã®å¿œç­”
            filepath: ä¿å­˜å…ˆJSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            importance: é‡è¦åº¦ (0.0-1.0)
        
        Returns:
            è¨˜æ†¶ã®UUID
        """
        memories = self.load_memories(filepath)
        
        memory_id = str(uuid.uuid4())
        memory = {
            "id": memory_id,
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "response": response,
            "vector": self._encode_vector(vector),
            "importance": importance
        }
        
        memories.append(memory)
        
        # ä¿å­˜ (æœ€å¤§100ä»¶ã«åˆ¶é™ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„)
        if len(memories) > 100:
            # é‡è¦åº¦ã®ä½ã„å¤ã„è¨˜æ†¶ã‹ã‚‰å‰Šé™¤
            memories.sort(key=lambda m: (m.get("importance", 0), m.get("timestamp", "")))
            memories = memories[-100:]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(memories, f, ensure_ascii=False, indent=2)
        
        print(f"             [LTM]: ğŸ’¾ Memory Saved (ID: {memory_id[:8]}...)")
        return memory_id
    
    def load_memories(self, filepath: str) -> List[Dict]:
        """LTMãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨è¨˜æ†¶ã‚’èª­ã¿è¾¼ã‚€"""
        if not os.path.exists(filepath):
            return []
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return []
    
    def recall(
        self, 
        query_vector: np.ndarray, 
        filepath: str, 
        top_k: int = 3,
        similarity_threshold: float = 0.3
    ) -> List[Tuple[Dict, float]]:
        """
        é¡ä¼¼è¨˜æ†¶ã‚’æ¤œç´¢ã—ã¦æƒ³èµ·ã™ã‚‹ã€‚
        
        Args:
            query_vector: æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ãªã‚‹æ€è€ƒãƒ™ã‚¯ãƒˆãƒ«
            filepath: LTMãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            top_k: è¿”ã™è¨˜æ†¶ã®æœ€å¤§æ•°
            similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤
        
        Returns:
            [(è¨˜æ†¶Dict, é¡ä¼¼åº¦), ...] ã®ãƒªã‚¹ãƒˆï¼ˆé¡ä¼¼åº¦é™é †ï¼‰
        """
        memories = self.load_memories(filepath)
        if not memories:
            return []
        
        results = []
        for mem in memories:
            try:
                stored_vec = self._decode_vector(mem["vector"])
                sim = self.cosine_similarity(query_vector, stored_vec)
                if sim >= similarity_threshold:
                    results.append((mem, sim))
            except (KeyError, ValueError):
                continue  # ç ´æã—ãŸè¨˜æ†¶ã¯ã‚¹ã‚­ãƒƒãƒ—
        
        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½Kä»¶ã‚’è¿”ã™
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

